#!/usr/bin/env python3
"""
ë§ˆë¼í†¤ì˜¨ë¼ì¸(marathon.pe.kr) êµ­ë‚´ ë§ˆë¼í†¤ ëŒ€íšŒ í¬ë¡¤ë§
ëª©ë¡ + ìƒì„¸í˜ì´ì§€ â†’ marathons_korea.json
"""

import requests
import json
import re
import time
from datetime import datetime
from urllib.parse import urljoin
from bs4 import BeautifulSoup


BASE_URL = "http://www.roadrun.co.kr/schedule"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
}


def fetch_html(url):
    """EUC-KR í˜ì´ì§€ë¥¼ UTF-8 ë¬¸ìì—´ë¡œ ë°˜í™˜"""
    resp = requests.get(url, headers=HEADERS, timeout=15)
    resp.encoding = "euc-kr"
    return resp.text


def parse_list_page():
    """ëª©ë¡ í˜ì´ì§€ì—ì„œ ëŒ€íšŒ ê¸°ë³¸ ì •ë³´ + ID ì¶”ì¶œ"""
    html = fetch_html(f"{BASE_URL}/list.php")
    soup = BeautifulSoup(html, "html.parser")

    events = []
    seen_ids = set()

    # view.php?no=XXXXX ë§í¬ì—ì„œ IDì™€ ëŒ€íšŒëª… ì¶”ì¶œ
    for a_tag in soup.find_all("a", href=re.compile(r"view\.php\?no=\d+")):
        href = a_tag["href"]
        m = re.search(r"no=(\d+)", href)
        if not m:
            continue
        eid = m.group(1)
        if eid in seen_ids:
            continue
        seen_ids.add(eid)

        title = a_tag.get_text(strip=True)
        if not title:
            continue

        # ì¢…ëª©: ëŒ€íšŒëª… ë°”ë¡œ ë‹¤ìŒ <font size="2" color="#990000">
        distances = ""
        next_font = a_tag.find_next("font", attrs={"color": "#990000"})
        if next_font:
            distances = next_font.get_text(strip=True)

        # ë‚ ì§œ: ì´ í–‰ ìœ„ìª½ì˜ ë‚ ì§œ ì…€ (M/D í˜•ì‹)
        tr = a_tag.find_parent("tr")
        date_str = ""
        day_of_week = ""
        if tr:
            # ê°™ì€ tr ë‚´ ì²« tdì— ë‚ ì§œê°€ ìˆê±°ë‚˜, ì´ì „ trì— ìˆìŒ
            date_td = tr.find("td", width="18%")
            if date_td:
                date_text = date_td.get_text(strip=True)
                dm = re.search(r"(\d{1,2}/\d{1,2})", date_text)
                if dm:
                    date_str = dm.group(1)
                dw = re.search(r"\((.)\)", date_text)
                if dw:
                    day_of_week = dw.group(1)

        # ì¥ì†Œ: width="19%" td
        location = ""
        if tr:
            loc_td = tr.find("td", width="19%")
            if loc_td:
                location = loc_td.get_text(strip=True)

        # ì£¼ìµœ / ì „í™”: width="30%" td
        organizer = ""
        phone = ""
        if tr:
            org_td = tr.find("td", width="30%")
            if org_td:
                org_text = org_td.get_text(" ", strip=True)
                # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
                pm = re.search(r"â˜?([\d\-]+)", org_text)
                if pm:
                    phone = pm.group(1)
                # ì£¼ìµœ: ì „í™” ì• í…ìŠ¤íŠ¸
                org_parts = org_text.split("â˜")[0].strip()
                if org_parts:
                    organizer = org_parts

        # í™ˆí˜ì´ì§€ ë§í¬
        website = ""
        if tr:
            home_a = tr.find("a", href=re.compile(r"^http"), target="_new")
            if home_a:
                website = home_a["href"]

        events.append({
            "id": eid,
            "title": title,
            "date": date_str,
            "dayOfWeek": day_of_week,
            "distances": distances,
            "location": location,
            "organizer": organizer,
            "phone": phone,
            "website": website,
        })

    return events


def parse_detail_page(eid):
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ"""
    html = fetch_html(f"{BASE_URL}/view.php?no={eid}")
    soup = BeautifulSoup(html, "html.parser")

    detail = {}
    field_map = {
        "ëŒ€íšŒëª…": "title",
        "ëŒ€í‘œìëª…": "representative",
        "E-mail": "email",
        "ëŒ€íšŒì¼ì‹œ": "datetime",
        "ì „í™”ë²ˆí˜¸": "phone",
        "ëŒ€íšŒì¢…ëª©": "distances",
        "ëŒ€íšŒì§€ì—­": "region",
        "ëŒ€íšŒì¥ì†Œ": "venue",
        "ì£¼ìµœë‹¨ì²´": "organizer",
        "ì ‘ìˆ˜ê¸°ê°„": "registrationPeriod",
        "í™ˆí˜ì´ì§€": "website",
        "ê¸°íƒ€ì†Œê°œ": "description",
    }

    rows = soup.find_all("tr")
    for row in rows:
        cells = row.find_all("td")
        if len(cells) < 2:
            continue
        label = cells[0].get_text(strip=True)
        if label in field_map:
            key = field_map[label]
            if key == "description":
                # HTML ì¤„ë°”ê¿ˆ ë³´ì¡´
                value = cells[1].decode_contents()
                value = re.sub(r"<br\s*/?>", "\n", value)
                value = re.sub(r"<[^>]+>", "", value)
                value = re.sub(r"&nbsp;", " ", value)
                value = value.strip()
            elif key == "website":
                a = cells[1].find("a", href=True)
                value = a["href"] if a else cells[1].get_text(strip=True)
            elif key == "email":
                a = cells[1].find("a", href=True)
                if a:
                    em = re.search(r"mail_url=([^'\"&]+)", a.get("href", ""))
                    value = em.group(1) if em else cells[1].get_text(strip=True)
                else:
                    value = cells[1].get_text(strip=True)
            else:
                value = cells[1].get_text(strip=True)
            detail[key] = value

    # ê¸°íƒ€ì†Œê°œì—ì„œ ê°€ê²© ì¶”ì¶œ
    desc = detail.get("description", "")
    if desc:
        # "ì°¸ê°€ë¹„" ì£¼ë³€ ê¸ˆì•¡ ìš°ì„  íƒìƒ‰
        price_section = re.search(r"ì°¸ê°€ë¹„[^\d]{0,30}([\d,]+)\s*ì›", desc)
        if price_section:
            detail["price"] = price_section.group(1).replace(",", "") + "ì›"
        else:
            # ê·¸ ì™¸ "XX,XXXì›" íŒ¨í„´ ì¤‘ ì²« ë²ˆì§¸
            price_match = re.search(r"([\d,]+)\s*ì›", desc)
            if price_match:
                raw_price = price_match.group(1).replace(",", "")
                # ë„ˆë¬´ í° ìˆ˜(ë‚ ì§œ ë“± ì˜¤íƒ)ë‚˜ ë„ˆë¬´ ì‘ì€ ìˆ˜ ì œì™¸
                if raw_price.isdigit() and 1000 <= int(raw_price) <= 1000000:
                    detail["price"] = raw_price + "ì›"

    return detail


def is_image_url(url):
    """URLì´ ì´ë¯¸ì§€ íŒŒì¼ í™•ì¥ìì¸ì§€ í™•ì¸"""
    return bool(re.search(r"\.(jpg|jpeg|png|webp|gif|bmp|svg)(\?|$|#)", url, re.I))


def fetch_hero_image(website_url):
    """ëŒ€íšŒ í™ˆí˜ì´ì§€ì—ì„œ ëŒ€í‘œ ì´ë¯¸ì§€ URL ì¶”ì¶œ (ì´ë¯¸ì§€ íŒŒì¼ë§Œ, main ìš°ì„ )"""
    if not website_url:
        return ""
    try:
        resp = requests.get(website_url, headers=HEADERS, timeout=10, allow_redirects=True)
        resp.encoding = resp.apparent_encoding
        html = resp.text
        soup = BeautifulSoup(html, "html.parser")

        # í˜ì´ì§€ ë‚´ ëª¨ë“  ì´ë¯¸ì§€ URL ìˆ˜ì§‘
        all_images = []

        # og:image
        og = soup.find("meta", property="og:image")
        if og and og.get("content"):
            all_images.append(urljoin(website_url, og["content"]))

        # twitter:image
        tw = soup.find("meta", attrs={"name": "twitter:image"})
        if tw and tw.get("content"):
            all_images.append(urljoin(website_url, tw["content"]))

        # CSS background-image
        for tag in soup.find_all(style=re.compile(r"background(-image)?\s*:")):
            style = tag.get("style", "")
            bg = re.search(r"url\(['\"]?([^'\")]+)['\"]?\)", style)
            if bg:
                all_images.append(urljoin(website_url, bg.group(1)))

        # <img> íƒœê·¸
        for img in soup.find_all("img", src=True):
            src = img["src"]
            if re.search(r"(icon|btn|button|arrow|sprite|pixel|spacer|1x1|blank)", src, re.I):
                continue
            all_images.append(urljoin(website_url, src))

        # ì´ë¯¸ì§€ í™•ì¥ì í•„í„°
        all_images = [url for url in all_images if is_image_url(url)]
        if not all_images:
            return ""

        # "main"ì´ í¬í•¨ëœ ì´ë¯¸ì§€ ìš°ì„ 
        banners = [url for url in all_images if re.search(r"banner", url, re.I)]
        mains = [url for url in all_images if re.search(r"main", url, re.I)]
        logos = [url for url in all_images if re.search(r"logo", url, re.I)]

        main_images = banners + mains + logos

        if main_images:
            return main_images[0]

        # ê·¸ ì™¸ ì²« ë²ˆì§¸ ì´ë¯¸ì§€
        return all_images[0]

    except Exception:
        return ""


ALLOWED_DISTANCES = {"í’€", "í•˜í”„", "10km", "5km"}


def filter_distances(distances_str):
    """í—ˆìš©ëœ ì¢…ëª©(í’€, í•˜í”„, 10km, 5km)ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°"""
    if not distances_str:
        return ""
    parts = [d.strip() for d in distances_str.split(",")]
    filtered = [d for d in parts if d in ALLOWED_DISTANCES]
    return ",".join(filtered)


def normalize_date(date_str, year=2026):
    """'2/8' ë˜ëŠ” '2026ë…„2ì›”8ì¼' â†’ '2026-02-08'"""
    # ëª©ë¡ í˜ì´ì§€ í˜•ì‹: M/D
    m = re.match(r"(\d{1,2})/(\d{1,2})", date_str)
    if m:
        month, day = int(m.group(1)), int(m.group(2))
        return f"{year}-{month:02d}-{day:02d}"

    # ìƒì„¸ í˜ì´ì§€ í˜•ì‹: YYYYë…„Mì›”Dì¼
    m = re.match(r"(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼", date_str)
    if m:
        return f"{m.group(1)}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"

    return date_str


def parse_registration_dates(period_str):
    """'2025ë…„10ì›”28ì¼~2026ë…„2ì›”1ì¼' â†’ ('2025-10-28', '2026-02-01')"""
    if not period_str:
        return ("", "")
    parts = period_str.split("~")
    if len(parts) != 2:
        return (period_str, "")
    start = normalize_date(parts[0].strip())
    end = normalize_date(parts[1].strip())
    return (start, end)


def main():
    print("=" * 60)
    print("ğŸƒ ë§ˆë¼í†¤ì˜¨ë¼ì¸ êµ­ë‚´ ëŒ€íšŒ í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 60)

    # 1) ëª©ë¡ íŒŒì‹±
    print("\nğŸ“¡ ëª©ë¡ í˜ì´ì§€ íŒŒì‹± ì¤‘...")
    events = parse_list_page()
    print(f"   {len(events)}ê°œ ëŒ€íšŒ ë°œê²¬")

    # 2) ì˜¤ëŠ˜ ì´ì „ ëŒ€íšŒ ì œì™¸
    today = datetime.now().strftime("%Y-%m-%d")
    filtered = []
    for ev in events:
        d = normalize_date(ev["date"])
        if d >= today:
            ev["dateFormatted"] = d
            filtered.append(ev)
    print(f"   ì˜¤ëŠ˜({today}) ì´í›„ ëŒ€íšŒ: {len(filtered)}ê°œ")

    # 3) ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
    print(f"\nğŸ“¡ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘... (ì´ {len(filtered)}ê°œ)")
    results = []
    for i, ev in enumerate(filtered, 1):
        try:
            detail = parse_detail_page(ev["id"])
            time.sleep(0.3)

            # ìƒì„¸ì—ì„œ ë‚ ì§œ ì •ê·œí™”
            dt_raw = detail.get("datetime", "")
            start_time = ""
            tm = re.search(r"ì¶œë°œì‹œê°„:\s*(\d{1,2}:\d{2})", dt_raw)
            if tm:
                start_time = tm.group(1)

            # í™ˆí˜ì´ì§€ì—ì„œ ëŒ€í‘œ ì´ë¯¸ì§€ ì¶”ì¶œ
            site_url = detail.get("website", ev["website"])
            image = ""
            if site_url:
                image = fetch_hero_image(site_url)
                time.sleep(0.3)

            merged = {
                "id": ev["id"],
                "title": detail.get("title", ev["title"]),
                "date": ev.get("dateFormatted", ev["date"]),
                "dayOfWeek": ev["dayOfWeek"],
                "startTime": start_time,
                "distances": filter_distances(detail.get("distances", ev["distances"])),
                "region": detail.get("region", ""),
                "venue": detail.get("venue", ev["location"]),
                "organizer": detail.get("organizer", ev["organizer"]),
                "representative": detail.get("representative", ""),
                "phone": detail.get("phone", ev["phone"]),
                "email": detail.get("email", ""),
                "website": site_url,
                "registrationStartDate": parse_registration_dates(detail.get("registrationPeriod", ""))[0],
                "registrationEndDate": parse_registration_dates(detail.get("registrationPeriod", ""))[1],
                "price": detail.get("price", ""),
                "description": detail.get("description", ""),
                "image": image,
            }
            results.append(merged)

            if i % 10 == 0:
                print(f"   {i}/{len(filtered)} ì™„ë£Œ")

        except Exception as e:
            print(f"   âš ï¸ {ev['id']} ({ev['title']}) ì‹¤íŒ¨: {e}")
            continue

    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ")

    # 4) ì €ì¥
    output = {
        "metadata": {
            "total_count": len(results),
            "fetched_at": datetime.now().isoformat(),
            "source": "http://www.marathon.pe.kr/index_calendar.html",
        },
        "marathons": results,
    }

    with open("marathons_korea.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ marathons_korea.json ì €ì¥ ì™„ë£Œ ({len(results)}ê°œ)")

    # ìƒ˜í”Œ ì¶œë ¥
    print("\nğŸ“‹ ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
    for i, m in enumerate(results[:3], 1):
        print(f"\n{i}. {m['title']}")
        print(f"   ë‚ ì§œ: {m['date']} ({m['dayOfWeek']}) {m['startTime']}")
        print(f"   ì¢…ëª©: {m['distances']}")
        print(f"   ì¥ì†Œ: {m['venue']}")
        print(f"   ì£¼ìµœ: {m['organizer']}")
        print(f"   í™ˆí˜ì´ì§€: {m['website']}")
        print(f"   ì´ë¯¸ì§€: {m['image']}")


if __name__ == "__main__":
    main()
